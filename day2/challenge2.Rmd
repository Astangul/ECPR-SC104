---
title: "Challenge 2: Scraping The Guardian's homepage"
author: Pablo Barbera
date: "August 1, 2017"
output: html_document
---

In this coding challenge, you will scrape the home page of [The Guardian](www.theguardian.com). Combining the techniques we have covered in the class so far, the goal is to produce a .csv dataset with the URL of each article that appears in the home page, its headline, and the text of the article.

Let's start by examining the homepage. Read the HTML code into R, and parse it.

```{r}
url <- "https://www.theguardian.com"

```

Now, write code to extract the headlines and the URLs for each article. If you're having trouble, feel free to switch to parsing the [RSS feed](https://www.theguardian.com) instead.

```{r}


```

By now you should have a data frame that has the headline and the URL for each article as the two variables. Let's prototype how you could scrape the text in the body of each of those URLs. Pick the first URL and write some code to get an object (let's call it 'text') that contains the text of the article.


```{r}


```

Now that the code works, write a loop that will generalize it to all the URLs in the homepage. Make sure you first create an empty variable in the dataframe (again, let's call it 'text', and that each iteration of the loop fills in the ith element of that vector with the text of the article.


```{r}


```
